# LLMâ€‘CLIâ€‘BUILDERâ€‘PPM

**Seamlessly stitch together local LLM weights (Ollama) and Python runtime
dependencies (PPM) in one portable tool.**

*Version 0.1 â€” generated 2025-08-07*

---

## ğŸ“¦ What is it?

`llm` is a tiny C++17 utility (`LLM-CLI-BUILDER-PPM.cpp`) that:

1. **Pulls** a model with `ollama pull <model>`  
2. **Installs** any required Python wheels via `ppm import`  
3. **Logs** provenance (SHAâ€‘256 digests, timestamps) into the shared ppm
   manifest so builds are fully reproducible.

End result: a single command gives you an immediately runnable model
environment:

```bash
llm build llama3:instruct --dep transformers==4.42.0 --dep accelerate -v
ollama run llama3:instruct          # now â€œjust worksâ€
```

---

## âœ¨ Features

| Component | Capability |
|-----------|------------|
| **Ollama** | Fetches GGUF/GGML weights; supports remote, local or custom tags |
| **PPM** | Installs wheels/tarballs into global persistent cache (`~/.cache/ppm`) |
| **Provenance** | Computes SHAâ€‘256 on both model weights & wheels; stores to SQLite |
| **Verbose / JSON** | All core `ppm` flags passâ€‘through (`--verbose`, `--json`, `--dry-run`) |
| **Dependencies** | `--dep` flags or a `requirements.txt` via `llm deps` |

---

## ğŸ”§ Build

### Prerequisites
* C++17 compiler (GCCÂ 9+, ClangÂ 12+, MSVCÂ 19.3+)
* Ollama installed & on `$PATH`
* PPMÂ 6.0.0+ (CPU or GPU build)

### Compile

```bash
g++ -std=c++17 -Iinclude -Llib -lppm_core \
    -o llm src/LLM-CLI-BUILDER-PPM.cpp
```

### Optional: CMake

```cmake
find_package(ppm_core REQUIRED)
add_executable(llm src/LLM-CLI-BUILDER-PPM.cpp)
target_link_libraries(llm PRIVATE ppm_core)
```

---

## ğŸš€ Usage

```text
llm build <model> [--dep <spec>]... [--verbose]
llm deps  <requirements.txt> [--verbose]

Options:
  --dep <name[==ver]>   Extra Python dependency to install
  -v, --verbose         Chatty output
  -h, --help            Print help
```

Examples:

```bash
# Basic
llm build llama3

# Exact transformers version
llm build llama3:instruct --dep transformers==4.42.0

# Install deps from requirements file
llm deps requirements.txt -v
```

---

## ğŸ–¥ï¸ GPU Integrity Check

If `ppm_gpu` exists and CUDA is available, every downloaded file (wheel or
tarball) is hashed in parallel on the GPU, providing 10â€“20Ã— speedups for large
libraries.

---

## ğŸ—„ï¸ Cache Layout

```
~/.cache/ppm/
â”œâ”€â”€ wheels/
â”‚   â””â”€â”€ transformers/4.42.0/<sha256>.whl
â””â”€â”€ models/
    â””â”€â”€ llama3/<sha256>/...
```

Weights and Python packages live sideâ€‘byâ€‘side: no duplication, easy cleanup.

---

## ğŸ§© Embedding in Your Project

### Python hook

```python
import subprocess
subprocess.check_call(["llm", "build", "mistral"])
```

### C++ call

```cpp
#include <cstdlib>
int main() {
    return std::system("llm build llama3:8b -v");
}
```

---

## âš™ï¸ Configuration

`~/.config/ppm/ppm.toml` supports:

```toml
cache_dir = "/mnt/ssd/ppm"
index_url = "https://pypi.org/simple"
retries   = 3
gpu.hash  = "sha256"
```

---

## ğŸ§‘â€ğŸ’» Contributing

1. Fork repo & open PR against `main`.  
2. Ensure `make test` passes (GoogleTest suite).  
3. Sign commits (`git commit -s`).  

---

## ğŸ“„ License

MITÂ License â€” Â©Â 2025 Dr. Josef KurkÂ Edwards & Contributors.
